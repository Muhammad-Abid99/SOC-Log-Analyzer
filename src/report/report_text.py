# SPDX-FileCopyrightText: 2025 G. Mohammad <ghmuhammad324@gmail.com>
# SPDX-License-Identifier: Apache-2.0

# src/report/report_text.py

import datetime
from textwrap import shorten

def generate_text_report(summary: dict, grouped_alerts: list, raw_alerts: list, output_dir: str) -> None:
    """
    Generates a professional, enhanced text summary report for SOC-Log-Analyzer.

    Args:
        summary (dict): Summary stats (total_logs, top_users, etc.)
        grouped_alerts (list): Alerts grouped by type, user, severity.
        raw_alerts (list): Full raw alert dicts.
        output_dir (str): Path to save the text report file.
    """

    # Calculate duration
    start_time = summary.get("start_time")
    end_time = summary.get("end_time")
    try:
        start_dt = datetime.datetime.fromisoformat(start_time)
        end_dt = datetime.datetime.fromisoformat(end_time)
        duration_sec = (end_dt - start_dt).total_seconds()
        hours = int(duration_sec // 3600)
        minutes = int((duration_sec % 3600) // 60)
        duration_str = f"{hours}h {minutes}m"
    except Exception:
        duration_str = "Unknown"

    # Severity counts
    severity_counts = {}
    for g in grouped_alerts:
        sev = str(g.get("severity", "Unknown")).capitalize()
        severity_counts[sev] = severity_counts.get(sev, 0) + g.get("count", 1)

    # Potential anomalies
    anomalies = []
    for g in grouped_alerts:
        if "unusual logon time" in g.get("type", "").lower():
            anomalies.append(f"- Off-hours logon: User '{g.get('user', 'Unknown')}' ({g.get('count', 0)} times)")
    rare_events = [eid for eid, count in summary.get("top_event_ids", {}).items() if count <= 2]
    if rare_events:
        anomalies.append(f"- Rare events detected: {', '.join(str(e) for e in rare_events)}")

    # Start report
    lines = []
    lines.append("=" * 60)
    lines.append("SOC INCIDENT SUMMARY REPORT")
    lines.append(f"Report Generated At: {datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
    lines.append("=" * 60)
    lines.append("")
    
    # Dataset Overview
    lines.append("📊 Dataset Overview")
    lines.append(f"- Total Logs: {summary.get('total_logs', 'Unknown')}")
    lines.append(f"- Time Range: {start_time or 'Unknown'} → {end_time or 'Unknown'}")
    lines.append(f"- Duration: {duration_str}")
    lines.append(f"- Unique Hosts: {summary.get('unique_hosts', 'Unknown')}")
    lines.append("")

    # Severity Breakdown
    lines.append("🚨 Severity Breakdown")
    for sev, count in severity_counts.items():
        lines.append(f"- {sev}: {count}")
    lines.append("")

    # Grouped Alerts Summary
    lines.append("📌 Grouped Alerts Summary")
    if grouped_alerts:
        lines.append(f"{'Alert Type':30} | {'User':20} | {'Sev':6} | {'Cnt':3} | {'First Seen':16} | {'Last Seen':16}")
        lines.append("-" * 100)
        for g in grouped_alerts:
            first_seen = (str(g.get('first_seen'))[:16]) if g.get('first_seen') else "Unknown"
            last_seen = (str(g.get('last_seen'))[:16]) if g.get('last_seen') else "Unknown"

            lines.append(
                f"{shorten(g.get('type', 'Unknown'), 30):30} | "
                f"{shorten(g.get('user', 'Unknown'), 20):20} | "
                f"{str(g.get('severity', 'Unknown'))[:6]:6} | "
                f"{g.get('count', 0):3} | "
                f"{first_seen:16} | "
                f"{last_seen:16}"
            )
    else:
        lines.append("- No alerts triggered.")
    lines.append("")

    # Potential Anomalies
    lines.append("⚠️ Potential Anomalies")
    if anomalies:
        lines.extend(anomalies)
    else:
        lines.append("- None detected.")
    lines.append("")

    # Final Stats
    lines.append(f"Total Detection Alerts: {len(raw_alerts)}")
    lines.append("=" * 60)
    lines.append("Generated by SOC-Log-Analyzer | For Internal SOC Use Only")
    lines.append("Visit https://github.com/Muhammad-Abid99/SOC-Log-Analyzer for more info.")

    # Save to file
    report_path = f"{output_dir}/log_summary.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

